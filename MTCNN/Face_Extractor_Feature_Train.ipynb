{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-4-78d2a8555f4a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mPIL\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mImage\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorchvision\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtransforms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\envs\\yolo_v3\\lib\\site-packages\\torch\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    194\u001B[0m     \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_C\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_C\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    195\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 196\u001B[1;33m __all__ += [name for name in dir(_C)\n\u001B[0m\u001B[0;32m    197\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;34m'_'\u001B[0m \u001B[1;32mand\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    198\u001B[0m             not name.endswith('Base')]\n",
      "\u001B[1;31mNameError\u001B[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from src.get_nets import PNet, RNet, ONet\n",
    "from src.box_utils import nms, calibrate_box, get_image_boxes, convert_to_square\n",
    "from src.first_stage import run_first_stage\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../Dataset/emotiw/'\n",
    "\n",
    "processed_dataset_path = '../Dataset/FaceFeatures/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTCNN Model Definition for Extracting Face Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = PNet()\n",
    "rnet = RNet()\n",
    "onet = ONet()\n",
    "onet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnetFeatures(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(OnetFeatures, self).__init__()\n",
    "        self.features = nn.Sequential(*list(onet.children())[:-3])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "def get_face_features(image, min_face_size=20.0,\n",
    "                 thresholds=[0.6, 0.7, 0.8],\n",
    "                 nms_thresholds=[0.7, 0.7, 0.7]):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        image: an instance of PIL.Image.\n",
    "        min_face_size: a float number.\n",
    "        thresholds: a list of length 3.\n",
    "        nms_thresholds: a list of length 3.\n",
    "\n",
    "    Returns:\n",
    "        two float numpy arrays of shapes [n_boxes, 4] and [n_boxes, 10],\n",
    "        bounding boxes and facial landmarks.\n",
    "    \"\"\"\n",
    "\n",
    "    # LOAD MODELS\n",
    "    pnet = PNet()\n",
    "    rnet = RNet()\n",
    "    onet = ONet()\n",
    "    onet.eval()\n",
    "\n",
    "    # BUILD AN IMAGE PYRAMID\n",
    "    width, height = image.size\n",
    "    min_length = min(height, width)\n",
    "\n",
    "    min_detection_size = 12\n",
    "    factor = 0.707  # sqrt(0.5)\n",
    "\n",
    "    # scales for scaling the image\n",
    "    scales = []\n",
    "\n",
    "    # scales the image so that\n",
    "    # minimum size that we can detect equals to\n",
    "    # minimum face size that we want to detect\n",
    "    m = min_detection_size/min_face_size\n",
    "    min_length *= m\n",
    "\n",
    "    factor_count = 0\n",
    "    while min_length > min_detection_size:\n",
    "        scales.append(m*factor**factor_count)\n",
    "        min_length *= factor\n",
    "        factor_count += 1\n",
    "\n",
    "    # STAGE 1\n",
    "\n",
    "    # it will be returned\n",
    "    bounding_boxes = []\n",
    "\n",
    "    # run P-Net on different scales\n",
    "    for s in scales:\n",
    "        boxes = run_first_stage(image, pnet, scale=s, threshold=thresholds[0])\n",
    "        bounding_boxes.append(boxes)\n",
    "\n",
    "    # collect boxes (and offsets, and scores) from different scales\n",
    "    bounding_boxes = [i for i in bounding_boxes if i is not None]\n",
    "    bounding_boxes = np.vstack(bounding_boxes)\n",
    "\n",
    "    keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0])\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "\n",
    "    # use offsets predicted by pnet to transform bounding boxes\n",
    "    bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\n",
    "    # shape [n_boxes, 5]\n",
    "\n",
    "    bounding_boxes = convert_to_square(bounding_boxes)\n",
    "    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "\n",
    "    # STAGE 2\n",
    "\n",
    "    img_boxes = get_image_boxes(bounding_boxes, image, size=24)\n",
    "    img_boxes = Variable(torch.FloatTensor(img_boxes), volatile=True)\n",
    "    output = rnet(img_boxes)\n",
    "    offsets = output[0].data.numpy()  # shape [n_boxes, 4]\n",
    "    probs = output[1].data.numpy()  # shape [n_boxes, 2]\n",
    "\n",
    "    keep = np.where(probs[:, 1] > thresholds[1])[0]\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n",
    "    offsets = offsets[keep]\n",
    "\n",
    "    keep = nms(bounding_boxes, nms_thresholds[1])\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\n",
    "    bounding_boxes = convert_to_square(bounding_boxes)\n",
    "    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "\n",
    "    # STAGE 3\n",
    "\n",
    "    img_boxes = get_image_boxes(bounding_boxes, image, size=48)\n",
    "    if len(img_boxes) == 0: \n",
    "        return [], []\n",
    "    img_boxes = Variable(torch.FloatTensor(img_boxes), volatile=True)\n",
    "    output = onet(img_boxes)\n",
    "    \n",
    "    faceFeatureModel = OnetFeatures(onet)\n",
    "\n",
    "    featureOutputs = faceFeatureModel(img_boxes)\n",
    "    \n",
    "    landmarks = output[0].data.numpy()  # shape [n_boxes, 10]\n",
    "    offsets = output[1].data.numpy()  # shape [n_boxes, 4]\n",
    "    probs = output[2].data.numpy()  # shape [n_boxes, 2]\n",
    "\n",
    "    keep = np.where(probs[:, 1] > thresholds[2])[0]\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n",
    "    offsets = offsets[keep]\n",
    "    landmarks = landmarks[keep]\n",
    "\n",
    "    bounding_boxes = calibrate_box(bounding_boxes, offsets)\n",
    "    keep = nms(bounding_boxes, nms_thresholds[2], mode='min')\n",
    "    \n",
    "    featureOutputs = featureOutputs[keep]\n",
    "\n",
    "    return featureOutputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train and Val Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {x : datasets.ImageFolder(os.path.join(dataset_path, x))\n",
    "                    for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = image_datasets['train']\n",
    "validation_dataset = image_datasets['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train = sorted(os.listdir(dataset_path + 'train/Negative/'))\n",
    "neu_train = sorted(os.listdir(dataset_path + 'train/Neutral/'))\n",
    "pos_train = sorted(os.listdir(dataset_path + 'train/Positive/'))\n",
    "\n",
    "neg_val = sorted(os.listdir(dataset_path + 'val/Negative/'))\n",
    "neu_val = sorted(os.listdir(dataset_path + 'val/Neutral/'))\n",
    "pos_val = sorted(os.listdir(dataset_path + 'val/Positive/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train_filelist = [x.split('.')[0] for x in neg_train]\n",
    "neu_train_filelist = [x.split('.')[0] for x in neu_train]\n",
    "pos_train_filelist = [x.split('.')[0] for x in pos_train]\n",
    "\n",
    "neg_val_filelist = [x.split('.')[0] for x in neg_val]\n",
    "neu_val_filelist = [x.split('.')[0] for x in neu_val]\n",
    "pos_val_filelist = [x.split('.')[0] for x in pos_val]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train_filelist = neg_train_filelist[1:]\n",
    "\n",
    "neg_val_filelist = neg_val_filelist[1:]\n",
    "neu_val_filelist = neu_val_filelist[1:]\n",
    "pos_val_filelist = pos_val_filelist[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neg_train_filelist[:10])\n",
    "print(neu_train_filelist[:10])\n",
    "print(pos_train_filelist[:10])\n",
    "\n",
    "print(neg_val_filelist[:10])\n",
    "print(neu_val_filelist[:10])\n",
    "print(pos_val_filelist[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filelist = neg_train_filelist + neu_train_filelist + pos_train_filelist\n",
    "val_filelist = neg_val_filelist + neu_val_filelist + pos_val_filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_dataset))\n",
    "print(len(validation_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Face Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(validation_dataset)):\n",
    "    image, label = validation_dataset[i]\n",
    "    print(val_filelist[i])\n",
    "    try:\n",
    "        if label == 0:\n",
    "            if os.path.isfile(processed_dataset_path + 'val/Negative/' + val_filelist[i] + '.npz'):\n",
    "                print(val_filelist[i] + ' Already present')\n",
    "                continue\n",
    "            features = get_face_features(image)\n",
    "            if(type(features)) == tuple:\n",
    "                with open('hello.text', 'a') as f:\n",
    "                    f.write(val_filelist[i])\n",
    "                continue\n",
    "            features = features.data.numpy()\n",
    "\n",
    "            if features.size == 0:\n",
    "                print('MTCNN model handling empty face condition at ' + val_filelist[i])\n",
    "            np.savez(processed_dataset_path + 'val/Negative/' + val_filelist[i] , a=features)\n",
    "            \n",
    "        elif label == 1:\n",
    "            if os.path.isfile(processed_dataset_path + 'val/Neutral/' + val_filelist[i] + '.npz'):\n",
    "                print(val_filelist[i] + ' Already present')\n",
    "                continue\n",
    "            features = get_face_features(image)\n",
    "            if(type(features)) == tuple:\n",
    "                with open('hello.text', 'a') as f:\n",
    "                    f.write(val_filelist[i])\n",
    "                continue\n",
    "            features = features.data.numpy()\n",
    "            if features.size == 0:\n",
    "                print('MTCNN model handling empty face condition at ' + val_filelist[i])\n",
    "            np.savez(processed_dataset_path + 'val/Neutral/' + val_filelist[i] , a=features)\n",
    "            \n",
    "        else:\n",
    "            if os.path.isfile(processed_dataset_path + 'val/Positive/' + val_filelist[i] + '.npz'):\n",
    "                print(val_filelist[i] + ' Already present')\n",
    "                continue\n",
    "            features = get_face_features(image)\n",
    "            if(type(features)) == tuple:\n",
    "                with open('hello.text', 'a') as f:\n",
    "                    f.write(val_filelist[i])\n",
    "                continue\n",
    "            features = features.data.numpy()\n",
    "            if features.size == 0:\n",
    "                print('MTCNN model handling empty face condition at ' + val_filelist[i])\n",
    "            np.savez(processed_dataset_path + 'val/Positive/' + val_filelist[i] , a=features)            \n",
    "            \n",
    "    except ValueError:\n",
    "        print('No faces detected for ' + val_filelist[i] + \". Also MTCNN failed.\")\n",
    "        if label == 0:\n",
    "            np.savez(processed_dataset_path + 'val/Negative/' + val_filelist[i] , a=np.zeros(1))\n",
    "        elif label == 1:\n",
    "            np.savez(processed_dataset_path + 'val/Neutral/' + val_filelist[i] , a=np.zeros(1))\n",
    "        else:\n",
    "            np.savez(processed_dataset_path + 'val/Positive/' + val_filelist[i] , a=np.zeros(1))\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-root-py",
   "language": "python",
   "display_name": "Python [conda root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}